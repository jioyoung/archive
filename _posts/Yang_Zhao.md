I work in the department of econometrics and time-series at SAS Institute, Inc. As a research statistician developer, I conduct research into the sequential Monte Carlo (which is also called particle filter) methods and independently develop the corresponding tools from scratch in the SAS cloud platform--Viya. The particle filter product complements the Kalman filter and HMM products. The SAS Kalman Filter product only deals with the linear Gaussian state space model while the HMM product only supports discrete latent variables. For nonlinear or nonGaussian state space models, the particle filter is the only solution in most cases. An important application of this product is in the field of finance. In risk management, asset pricing, and portfolio optimization, volatility forecasting is critical. The stochastic volatility model like Heston or Autoregressive Stochastic Volatility models are the most popular tools for volatility estimation. However, there was no closed-form for the likelihood function so that the maximum likelihood estimation method fails. The particle filter provides a simulation-based solution and increasing computation power nowadays makes this method more and more efficient.

In the Particle Filter product I developed, online and offline algorithms are implemented to estimate parameters of the nonlinear or non-Gaussian state-space models. Moreover, it provides solutions to the inference problems such that the filtering, smoothing results for the latent variables will be estimated via the corresponding algorithms. 

The particle filter is also combined with the Markov chain Monte Carlo method such that the likelihood is approximated by particle filter. This mixed algorithm is called Particle MCMC method which is quite flexible and can be used to learn almost all the state-space models. 


The distributed computation schemes in the cloud/hybrid environment are implemented. It results in dozens of times faster performance than open-source or business competitor products.



 <!-- During my Ph.D. career, I had extensive course training in statistics, simulation, and optimization. My research focused on risk management, volatility model, and Monte Carlo methods.  -->
  
# A Simple and Robust Approach for ExpectedShortfall Estimation
Though many parametric and non-parametric expected shortfall methods have been studied, selecting an accurate ES estimator is a challenge in practice, especially for a small sample. In light of the popular arithmetic average of exceedances, the given sample of losses, however, is not always large enough to give a robust estimator. Assuming the sample size is 250, the number of one year's observations, only the largest two losses are covered when estimating $\text{ES}$ at the ${99\%}$ confidence level and only the maximal one is valuable for $\text{ES}$ at the ${99.5\%}$ level. ES estimator is quite unstable especially for a heavy-tailed loss distribution where it is easily affected by whether infrequent losses would occur in the realized sample. Moreover, given such small samples, estimating GARCH models precisely is also difficult.

Normal distributions have many nice properties with the presence justified by Central Limit Theorem but they cannot properly capture the heavy-tailed behaviors that are common in the financial data. A model building approach with normal distributions usually suffers the `underestimation' problem. Heavy-tailed distributions such as $t$ and other stable distributions have also been considered but some inherent drawbacks impede their applications. For example, the sum of two $t$ distributed r.v.'s generally no longer follows a $t$ distribution. Moreover, even though the sum of two stable r.v.'s follows a stable distribution, there is usually no general explicit formula for its probability density function. 

In the first project, a tail-based normal approximation with explicit formulas is derived by matching a specific quantile and the mean excess square of the sample observations. This tail-based feature proves to be effective to alleviate the `underestimation' problem. To enhance the estimation accuracy, I also propose a regression-adjusted tail-based normal approximation based on the sample's tail weight. It shows the adjusted ES estimator is robust and efficient in the sense that it can be applied to various heavy-tailed distributions, such as student's $t$, lognormal, Gamma, Weibull, etc., and the errors are all small. Moreover, compared to two common ES estimators---one is the arithmetic average of excessive losses and the other is the extreme value theory estimator, the proposed estimator achieves smaller mean square errors for small samples, especially at high confidence levels. Our method is also proved to work well under linear transformations, which is useful in the portfolio management.




# On GARCH and Autoregressive StochasticVolatility Approaches for Market Calibration and Option Pricing

This project carries out a comprehensive comparison of the Gaussian generalized autoregressive conditional heteroskedasticity (GARCH) and autoregressive stochastic volatility (ARSV) models using S\&P 500 Index. To date, the risk-neutral GARCH(1, 1) and ARSV(1) models have not been compared in the previous empirical studies. This paper complements the literature by conducting a comprehensive comparison of the GARCH(1, 1) and ARSV(1) models regarding in-sample fitting and out-of-sample prediction capabilities under both the physical and risk-neutral measures. Under the physical measure, we calculate their log-likelihoods, test normality for the error terms and explore the one-step-ahead volatility forecast. On the other hand, under the risk-neutral measure, the option pricing errors of the original and implied versions of the two models are investigated. We find the ARSV(1) model outperforms the GARCH(1, 1) model in terms of the in-sample and out-of-sample performances under the physical  measure. Under the risk-neutral measure, these two models obtain considerably similar results when pricing call options while the ARSV(1) model performs substantially better for put options. Although the implied versions of these two models improve the in-sample pricing errors as expected, they are sensitive to the initial conditions and not robust for the out-of-sample predictions.  


The Locally Risk-Neutral Valuation Relationship assumes volatilities under the two measures above are equal (see \citet*{duan1995garch}). In practice, the risk-neutral measure, however, tends to have larger volatilities than the counterparts under the physical measure, which is referred to as the volatility risk premium phenomenon

# A Comparison of Option Pricing Models with Leverage Effect
The objective of this project is to compare the capacities of the GARCH(1, 1) and ARSV(1) models with the leverage effect to fit the observed option prices and to forecast option prices in the future. To capture the leverage effect, the ARSV(1) model just needs to introduce a negative correlation between the return and volatility innovation processes but the GARCH(1, 1) model
has to replace its volatility specification with other forms such as the Exponential GARCH (EGARCH) and Nonlinear Asymmetric GARCH (NGARCH) models. In addition, I propose a continuous-time option pricing model that captures the leverage effect by straightforwardly relating the volatility to the exponential decayed weighted
average (EDWA) cumulative asset return. It is similar to the Constant elasticity of variance model (CEV) in a way that the leverage effect is considered directly. We show that incorporating the leverage effect brings about substantial improvements to the GARCH(1, 1) and ARSV(1) models in terms of the in-sample and out-of-sample option pricing performances. Moreover, our Cumulative Return model with only two parameters dominates other sophisticated models for predicting call options and has as accurate results as others when it comes to pricing put options. The performance of
our model in an outlier day also further highlights its robustness.


My industrial and academic experiences demonstrate the knowledge of math/statistics and excellent programming skills.
